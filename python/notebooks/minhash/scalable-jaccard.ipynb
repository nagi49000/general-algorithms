{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "35f6b2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a1a765",
   "metadata": {},
   "source": [
    "# Scalable Jaccard neighbour finding\n",
    "\n",
    "## Jaccard clustering\n",
    "\n",
    "A standard technique of handling document similarity or clustering is to tokenize the data, and then cluster on the tokens of each document. On small scales, this is relatively easy to get running in a notebook environment.\n",
    "\n",
    "A tokenizer maps each document to a set of tokens\n",
    "\n",
    "$ t : d_i \\rightarrow \\{t(d_i)_j\\}_{j=1}^{n_i} = \\{t_{ij}\\}_{j=1}^{n_i}$\n",
    "\n",
    "A tokenizer can be as elaborate or as simple as one wishes. Perhaps the simplest is breaking up a document over sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d167211a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I', 'am', 'a', 'fish'], ['a', 'fish', 'swims']]\n"
     ]
    }
   ],
   "source": [
    "docs = [\"I am a fish\", \"a fish swims\"]\n",
    "tokenized_docs = [x.split(\" \") for x in docs]\n",
    "print(tokenized_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9c176d",
   "metadata": {},
   "source": [
    "Using the tokens, one can estimate how similar documents might be. A typical way of seeing the similarity is by a Jaccard similarity. This looks at (between a pair of documents) the number of common tokens divided by the number of all tokens\n",
    "\n",
    "$ J_{sim}(d_x, d_y) = \\frac{\\{t_{xj}\\}_{j=1}^{n_x} \\cap \\{t_{yj}\\}_{j=1}^{n_y}}{\\{t_{xj}\\}_{j=1}^{n_x} \\cup \\{t_{yj}\\}_{j=1}^{n_y}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ac82eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_sim(d_x, d_y):\n",
    "    return len(set(d_x) & set(d_y)) / len(set(d_x) | set(d_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c59afa",
   "metadata": {},
   "source": [
    "For identical sets, the Jaccard similarity will be 1. For sets with no tokens in common, the similarity will be 0. For sets with some tokens in common, the similarity will be between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96f33e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_sim(tokenized_docs[0], tokenized_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad854f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_sim(tokenized_docs[0], tokenized_docs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b79889",
   "metadata": {},
   "source": [
    "From the similarity, one can define a distance, the Jaccard distance by\n",
    "\n",
    "$J_{dist}(d_x, d_y) = 1 - J_{sim}(d_x, d_y)$.\n",
    "\n",
    "This distance function forms a proper metric [[1]](https://en.wikipedia.org/wiki/Metric_space), i.e. it observes the distance function rules:\n",
    "- $ J_{dist}(d_x, d_x) = 0 $\n",
    "- $ J_{dist}(d_x, d_y) > 0 \\textrm{  for  } d_x \\neq d_y $\n",
    "- $ J_{dist}(d_x, d_y) = J_{dist}(d_y, d_x) $\n",
    "- $ J_{dist}(d_x, d_z) \\leq J_{dist}(d_x, d_y) + J_{dist}(d_y, d_z) $\n",
    "\n",
    "which means that the distance metric can legitimately be used for clustering. One can play the usual games with hierarchical clustering etc. In particular, for simple clustering, one can find a documents neighbours by thresholding on the distance, and describing the neighbours of a document as the other documents with a threshold distance $T$ of that document:\n",
    "\n",
    "$ \\textrm{neighbours}(d_i) = \\{d_j | J_{dist}(d_i, d_j) < T \\}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "456e73d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighbours(this_doc, all_docs, threshold, tokenizer=lambda x: x.split(\" \")):\n",
    "    this_tokenized_docs = tokenizer(this_doc)\n",
    "    all_tokenized_docs = [tokenizer(x) for x in all_docs]\n",
    "    neighbours = [y[0] for y in zip(all_docs, all_tokenized_docs) if (1 - jaccard_sim(y[1], this_tokenized_docs)) < threshold]\n",
    "    return neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2b3f280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am a fish']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbours(\"I am a fish\", docs, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7eb1810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am a fish', 'a fish swims']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbours(\"I am a fish\", docs, 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e9cc66",
   "metadata": {},
   "source": [
    "The simple implementations above work well for small datasets. However, for large datasets, the computation of distances, or finding nearest neighbours, is $O(N^2)$ where $N$ is the number of documents. Depending on how the results are stored, storage can also be a $N^2 $ problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf25d85",
   "metadata": {},
   "source": [
    "## Upscaling to larger datasets\n",
    "\n",
    "The approach above (up to some approximations) can be replaced by $O(N)$ operations. The approach is described at a high technical level in [[2]](https://en.wikipedia.org/wiki/Locality-sensitive_hashing). The idea is that rather than trying to perform nearest neighbour on all the tokenized documents $O(N^2)$, one can try and do an intensive operation on all the documents to a small hash ($O(N)$ in compute, and a small $O(N)$ in storage), and then perform nearest neighbour from the hashes. Computing neighbours by comparing hashes is an extremely cheap $O(N^2)$ operation, so that it is effectively $O(1)$ when compared to all other processing.\n",
    "\n",
    "Whilst the overall theoreticl approach is called Locality-sensitive hashing (LSH), in implementation circles, the implementations come down to a hashing mechanism to statistically reproduce the distance (e.g. minhash for Jaccard [[3]](https://en.wikipedia.org/wiki/MinHash) [[5]](https://www.youtube.com/watch?v=96WOGPUgMfw), or simhash for cosine [[4]](https://en.wikipedia.org/wiki/SimHash)) and a way of implementing a threshold (rows and bands in a LSH scheme [[6]](https://www.youtube.com/watch?v=_1D35bN95Go))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb2ebdf",
   "metadata": {},
   "source": [
    "## Minhash\n",
    "\n",
    "The name minhash comes from the fact that one is performing a minimum over hashes. The idea behind the calculation is to calculate an integer (which will be a bucket index) that can be used to estimate the Jaccard similarity. Since this integer is a far smaller piece of information than the original document, the estimation is 'lossy', or more precisely (since the hashing operation is random) statistically represents the Jaccard similarity.\n",
    "\n",
    "The idea comes down to performing some kind of one-hot encoding, and using the encoding to form 'buckets' of data. A populated bucket can then be chosen at random to see if there is a match between a pair of documents. For instance, take the example of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9f5ae98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am a fish', 'a fish swims']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec495aaf",
   "metadata": {},
   "source": [
    "These can be label-encoded using a scheme (which is a form of tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26cec7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2, 3], [2, 3, 4]]\n"
     ]
    }
   ],
   "source": [
    "label_scheme = {\n",
    "    \"I\": 0,\n",
    "    \"am\": 1,\n",
    "    \"a\": 2,\n",
    "    \"fish\": 3,\n",
    "    \"swims\": 4\n",
    "}\n",
    "label_encoded_docs = [[label_scheme[y] for y in x.split(\" \")] for x in docs]\n",
    "print(label_encoded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b413a245",
   "metadata": {},
   "source": [
    "One can now choose one of the labels (at random, by considering a uniform random variable over all labels) and see if that label appears in each of the encoded documents. The probability of a randomly chosen label appearing in both encoded documents is the number of common labels, divided by all possible labels. This probability is precisely the Jaccard similarity. One therefore finds, that taking a random variable $L \\sim U[\\textrm{all labels}]$\n",
    "\n",
    "$P(l \\in t(d_x) \\cap l \\in t(d_y)) = J_{sim}(d_x, d_y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9b13540b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empirical probability of randomly chosen label being common = 0.37\n"
     ]
    }
   ],
   "source": [
    "all_labels = list(set(sum(label_encoded_docs, [])))\n",
    "# common_labels = [x for x in all_labels if x in label_encoded_docs[0] and x in label_encoded_docs[1]]\n",
    "# print(f\"probability of a randomly chosen label common in both docs = {len(common_labels)/len(all_labels)}\")\n",
    "\n",
    "n_sample = 1000\n",
    "common_label_hits = 0\n",
    "for i_sample in range(n_sample):\n",
    "    label = random.choices(all_labels)[0]\n",
    "    if label in label_encoded_docs[0] and label in label_encoded_docs[1]:\n",
    "        common_label_hits += 1\n",
    "print(f\"empirical probability of randomly chosen label being common = {common_label_hits/n_sample}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ce668e",
   "metadata": {},
   "source": [
    "This is the principal behind minhashing, although the implementation is a little different to accommodate an arbitrary number of documents, rather than the toy example of two documents above. In minhash lingo, one thinks of 'buckets' instead of 'labels', where each label is an index to a bucket that a document can be 'dropped into'.\n",
    "\n",
    "Minhashing usually using a text tokenizer called 'shingling'. However, one can use any tokenizer, and can use the same tokenizer one may have used in the small scale pure Jaccard analysis one may have done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9f2bda88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[699, 699]\n",
      "[356, 356]\n",
      "[2235, 2235]\n",
      "[585, 585]\n",
      "[2444, 2589]\n",
      "[2058, 2058]\n",
      "[200, 200]\n",
      "[1328, 680]\n",
      "[523, 3997]\n",
      "[2049, 5965]\n"
     ]
    }
   ],
   "source": [
    "def get_minhashed_docs(tokenized_docs):\n",
    "    # convert text tokens into 32 bit integers\n",
    "    def str_to_int(s):\n",
    "        return int(hashlib.sha1(s.encode(\"utf-8\")).hexdigest(), 16) % 2 ** 31\n",
    "    int_tokenized_docs = [[str_to_int(y) for y in x] for x in tokenized_docs]\n",
    "    # optimization tip; the code below totally flies with numba\n",
    "    # generate a random hash. The random choice of hash marries up to the idea above of\n",
    "    # choosing a random label/bucket ...\n",
    "    a, b = random.randint(1, 2 ** 31), random.randint(1, 2 ** 31) \n",
    "    big_prime_number = 7919  # modulo by a prime number, since integers modulo p form a finite field\n",
    "                             # only when p is prime, which helps us get more coverage over the buckets\n",
    "                             # and minimize collisions\n",
    "    random_hashed_docs = [[(a * y + b) % big_prime_number for y in x] for x in int_tokenized_docs]\n",
    "    # ... since a different random hash will pull out a different label/bucket under the min operation\n",
    "    minhashes = [min(x) for x in random_hashed_docs]\n",
    "    return minhashes\n",
    " \n",
    "for _ in range(10):\n",
    "    print(get_minhashed_docs(tokenized_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3debe35",
   "metadata": {},
   "source": [
    "In the above, sometimes the two minhashes are the same, and sometimes they are different. When they are the same, this corresponds to the min operation in the get_minhashed_function pulling out a hash corresponding to a common token. When they are different, the min has pulled out hashes corresponding to different tokens. From the same reasoning as the simple label encoding example above, one has\n",
    "\n",
    "$P(\\textrm{minhash}(t(d_x)) = \\textrm{minhash}(t(d_y))) = J_{sim}(d_x, d_y)$\n",
    "\n",
    "where (under a slight abuse of notation), the minhash function on a document is a random variable, and the same random hash is used on $d_x$ and $d_y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4bd38f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empirical probability of minhashes being the same = 0.402\n"
     ]
    }
   ],
   "source": [
    "n_sample = 1000\n",
    "common_label_hits = 0\n",
    "for i_sample in range(n_sample):\n",
    "    # generating n_sample minhash integers per document, which can be thought of\n",
    "    # as generating a (minhash) vector of n_sample integers per document\n",
    "    minhashed_docs = get_minhashed_docs(tokenized_docs)\n",
    "    if minhashed_docs[0] == minhashed_docs[1]:\n",
    "        common_label_hits += 1\n",
    "print(f\"empirical probability of minhashes being the same = {common_label_hits/n_sample}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860db204",
   "metadata": {},
   "source": [
    "In the above, the minhashing operation has been performed over only 2 documents, but there is nothing to stop running the operation over $N$ documents to generate a minhash for each doc, and then check for equalities in the resulting minhashes.\n",
    "\n",
    "Each set of minhashes is a realisation of a random variable (a choice of randomly selected $a, b$ above). In this sense, the neighbour checking is statistical, and only an approximate nearest neighbour.\n",
    "\n",
    "The hashing process is also a lossy operation. Taking n_sample to be small yields a poor approximation to the Jaccard similarity. However, taking n_sample to be large means that a large amount of processing has to occur, yielding long minhash vectors, partially negating the benefit if using minhashing for computational and storage gain. The choice of minhash vector length (i.e. n_sample) really feeds into the LSH scheme, but generally does reflect 'how approximate' the approximate nearest neighbour approach will be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872370b6",
   "metadata": {},
   "source": [
    "## Locality-sensitive hashing\n",
    "\n",
    "In the above analysis, there is a function that takes tokenized documents to minhash vectors. These minhash vectors are statistical samples representing the Jaccard similarity between the referenced documents, and can be used to (statistically) estimate the Jaccard similarity.\n",
    "\n",
    "However, to declare if a pair of documents are neighbours, one must apply a threshold to the Jaccard similarity. This is accomplished using 'rows and bands' over the minhash vectors. The underlying idea is that requiring more of the minhashes to be equal puts a stronger constraint on saying when two documents are neighbours, and therefore corresponds to a tighter/smaller threshold on the Jaccard distance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad4c965",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] https://en.wikipedia.org/wiki/Metric_space\n",
    "\n",
    "[2] https://en.wikipedia.org/wiki/Locality-sensitive_hashing\n",
    "\n",
    "[3] https://en.wikipedia.org/wiki/MinHash\n",
    "\n",
    "[4] https://en.wikipedia.org/wiki/SimHash\n",
    "\n",
    "[5] https://www.youtube.com/watch?v=96WOGPUgMfw\n",
    "\n",
    "[6] https://www.youtube.com/watch?v=_1D35bN95Go"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
