{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e538bc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8633f1b1",
   "metadata": {},
   "source": [
    "# Scalable Jaccard neighbour finding\n",
    "\n",
    "## Jaccard clustering\n",
    "\n",
    "A standard technique of handling document similarity or clustering is to tokenize the data, and then cluster on the tokens of each document. On small scales, this is relatively easy to get running in a notebook environment.\n",
    "\n",
    "A tokenizer maps each document to a set of tokens\n",
    "\n",
    "$ t : d_i \\rightarrow \\{t(d_i)_j\\}_{j=1}^{n_i} = \\{t_{ij}\\}_{j=1}^{n_i}$\n",
    "\n",
    "A tokenizer can be as elaborate or as simple as one wishes. Perhaps the simplest is breaking up a document over sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55e71d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I', 'am', 'a', 'fish'], ['a', 'fish', 'swims']]\n"
     ]
    }
   ],
   "source": [
    "docs = [\"I am a fish\", \"a fish swims\"]\n",
    "tokenized_docs = [x.split(\" \") for x in docs]\n",
    "print(tokenized_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204e051f",
   "metadata": {},
   "source": [
    "Using the tokens, one can estimate how similar documents might be. A typical way of seeing the similarity is by a Jaccard similarity. This looks at (between a pair of documents) the number of common tokens divided by the number of all tokens\n",
    "\n",
    "$ J_{sim}(d_x, d_y) = \\frac{|\\{t_{xj}\\}_{j=1}^{n_x} \\cap \\{t_{yj}\\}_{j=1}^{n_y}|}{|\\{t_{xj}\\}_{j=1}^{n_x} \\cup \\{t_{yj}\\}_{j=1}^{n_y}|} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0792a7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_sim(d_x, d_y):\n",
    "    return len(set(d_x) & set(d_y)) / len(set(d_x) | set(d_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfa5942",
   "metadata": {},
   "source": [
    "For identical sets, the Jaccard similarity will be 1. For sets with no tokens in common, the similarity will be 0. For sets with some tokens in common, the similarity will be between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9977d1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_sim(tokenized_docs[0], tokenized_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bebbadf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_sim(tokenized_docs[0], tokenized_docs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4e7f32",
   "metadata": {},
   "source": [
    "From the similarity, one can define a distance, the Jaccard distance by\n",
    "\n",
    "$J_{dist}(d_x, d_y) = 1 - J_{sim}(d_x, d_y)$.\n",
    "\n",
    "This distance function forms a proper metric [[1]](https://en.wikipedia.org/wiki/Metric_space), i.e. it observes the distance function rules:\n",
    "- $ J_{dist}(d_x, d_x) = 0 $\n",
    "- $ J_{dist}(d_x, d_y) > 0 \\textrm{  for  } d_x \\neq d_y $\n",
    "- $ J_{dist}(d_x, d_y) = J_{dist}(d_y, d_x) $\n",
    "- $ J_{dist}(d_x, d_z) \\leq J_{dist}(d_x, d_y) + J_{dist}(d_y, d_z) $\n",
    "\n",
    "which means that the distance metric can legitimately be used for clustering. One can play the usual games with hierarchical clustering etc. In particular, for simple clustering, one can find a documents neighbours by thresholding on the distance, and describing the neighbours of a document as the other documents with a threshold distance $T$ of that document:\n",
    "\n",
    "$ \\textrm{neighbours}(d_i) = \\{d_j | J_{dist}(d_i, d_j) < T \\}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64508b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighbours(this_doc, all_docs, threshold, tokenizer=lambda x: x.split(\" \")):\n",
    "    this_tokenized_docs = tokenizer(this_doc)\n",
    "    all_tokenized_docs = [tokenizer(x) for x in all_docs]\n",
    "    neighbours = [y[0] for y in zip(all_docs, all_tokenized_docs) if (1 - jaccard_sim(y[1], this_tokenized_docs)) < threshold]\n",
    "    return neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f63c707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am a fish']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbours(\"I am a fish\", docs, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c70144b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am a fish', 'a fish swims']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbours(\"I am a fish\", docs, 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bdbf58",
   "metadata": {},
   "source": [
    "The simple implementations above work well for small datasets. However, for large datasets, the computation of distances, or finding nearest neighbours, is $O(N^2)$ where $N$ is the number of documents. Depending on how the results are stored, storage can also be a $N^2 $ problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928c0507",
   "metadata": {},
   "source": [
    "## Upscaling to larger datasets\n",
    "\n",
    "The approach above (up to some approximations) can be replaced by $O(N)$ operations. The approach is described at a high technical level in [[2]](https://en.wikipedia.org/wiki/Locality-sensitive_hashing). A good in-depth technical overview of everything in this notebook (and then some) is excellently presented in [[8]](http://www.mmds.org/). The idea is that rather than trying to perform nearest neighbour on all the tokenized documents $O(N^2)$, one can try and do an intensive operation on all the documents to a small hash ($O(N)$ in compute, and a small $O(N)$ in storage), and then perform nearest neighbour from the hashes. Computing neighbours by comparing hashes is an extremely cheap $O(N^2)$ operation, so that it is effectively $O(1)$ when compared to all other processing.\n",
    "\n",
    "Whilst the overall theoreticl approach is called Locality-sensitive hashing (LSH), in implementation circles, the implementations come down to a hashing mechanism to statistically reproduce the distance (e.g. minhash for Jaccard [[3]](https://en.wikipedia.org/wiki/MinHash) [[5]](https://www.youtube.com/watch?v=96WOGPUgMfw), or simhash for cosine [[4]](https://en.wikipedia.org/wiki/SimHash)) and a way of implementing a threshold (rows and bands in a LSH scheme [[6]](https://www.youtube.com/watch?v=_1D35bN95Go))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e1af9b",
   "metadata": {},
   "source": [
    "## Minhash\n",
    "\n",
    "The name minhash comes from the fact that one is performing a minimum over hashes. The idea behind the calculation is to calculate an integer (which will be a bucket index) that can be used to estimate the Jaccard similarity. Since this integer is a far smaller piece of information than the original document, the estimation is 'lossy', or more precisely (since the hashing operation is random) statistically represents the Jaccard similarity.\n",
    "\n",
    "The idea comes down to performing some kind of one-hot encoding, and using the encoding to form 'buckets' of data. A populated bucket can then be chosen at random to see if there is a match between a pair of documents. For instance, take the example of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c464883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am a fish', 'a fish swims']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08efc7a5",
   "metadata": {},
   "source": [
    "These can be label-encoded using a scheme (which is a form of tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1251cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2, 3], [2, 3, 4]]\n"
     ]
    }
   ],
   "source": [
    "label_scheme = {\n",
    "    \"I\": 0,\n",
    "    \"am\": 1,\n",
    "    \"a\": 2,\n",
    "    \"fish\": 3,\n",
    "    \"swims\": 4\n",
    "}\n",
    "label_encoded_docs = [[label_scheme[y] for y in x.split(\" \")] for x in docs]\n",
    "print(label_encoded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7be8ddc",
   "metadata": {},
   "source": [
    "One can now choose one of the labels (at random, by considering a uniform random variable over all labels) and see if that label appears in each of the encoded documents. The probability of a randomly chosen label appearing in both encoded documents is the number of common labels, divided by all possible labels. This probability is precisely the Jaccard similarity. One therefore finds, that taking a random variable $L \\sim U[\\textrm{all labels}]$\n",
    "\n",
    "$P[l \\in t(d_x) \\cap l \\in t(d_y)] = J_{sim}(d_x, d_y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23f03d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empirical probability of randomly chosen label being common = 0.399\n"
     ]
    }
   ],
   "source": [
    "all_labels = list(set(sum(label_encoded_docs, [])))\n",
    "# common_labels = [x for x in all_labels if x in label_encoded_docs[0] and x in label_encoded_docs[1]]\n",
    "# print(f\"probability of a randomly chosen label common in both docs = {len(common_labels)/len(all_labels)}\")\n",
    "\n",
    "n_sample = 1000\n",
    "common_label_hits = 0\n",
    "for i_sample in range(n_sample):\n",
    "    label = random.choices(all_labels)[0]\n",
    "    if label in label_encoded_docs[0] and label in label_encoded_docs[1]:\n",
    "        common_label_hits += 1\n",
    "print(f\"empirical probability of randomly chosen label being common = {common_label_hits/n_sample}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afed4c76",
   "metadata": {},
   "source": [
    "This is the principal behind minhashing, although the implementation is a little different to accommodate an arbitrary number of documents, rather than the toy example of two documents above. In minhash lingo, one thinks of 'buckets' instead of 'labels', where each label is an index to a bucket that a document can be 'dropped into'.\n",
    "\n",
    "Minhashing usually using a text tokenizer called 'shingling'. However, one can use any tokenizer, and can use the same tokenizer one may have used in the small scale pure Jaccard analysis one may have done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f786f8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1433, 1393]\n",
      "[133, 2215]\n",
      "[3505, 821]\n",
      "[928, 2076]\n",
      "[1405, 1782]\n",
      "[2743, 3559]\n",
      "[4542, 4756]\n",
      "[2165, 4562]\n",
      "[256, 1187]\n",
      "[432, 432]\n"
     ]
    }
   ],
   "source": [
    "def get_minhashed_docs(tokenized_docs):\n",
    "    # convert text tokens into 32 bit integers\n",
    "    def str_to_int(s):\n",
    "        return int(hashlib.sha1(s.encode(\"utf-8\")).hexdigest(), 16) % 2 ** 31\n",
    "    int_tokenized_docs = [[str_to_int(y) for y in x] for x in tokenized_docs]\n",
    "    # optimization tip; the code below totally flies with numba\n",
    "    # using random numbers that fit into 32 bit integers, since then multiplicative arithmetic will be\n",
    "    #   valid over 64 bit integers, in case one needs a true fixed point implementation\n",
    "    # generate a random hash. The random choice of hash marries up to the idea above of\n",
    "    #   choosing a random label/bucket ...\n",
    "    a, b = random.randint(1, 2 ** 31), random.randint(1, 2 ** 31) \n",
    "    big_prime_number = 7919  # modulo by a prime number, as opposed to any other number,\n",
    "                             # since this guarantees that 'a' and 'p' are coprime,\n",
    "                             # which helps minimize collisions, and get more coverage over the buckets\n",
    "    p = big_prime_number\n",
    "    random_hashed_docs = [[(a * y + b) % p for y in x] for x in int_tokenized_docs]\n",
    "    # ... since a different random hash will pull out a different label/bucket under the min operation\n",
    "    minhashes = [min(x) for x in random_hashed_docs]\n",
    "    return minhashes\n",
    " \n",
    "for _ in range(10):\n",
    "    print(get_minhashed_docs(tokenized_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8743e8f",
   "metadata": {},
   "source": [
    "In the above, sometimes the two minhashes are the same, and sometimes they are different. When they are the same, this corresponds to the min operation in the get_minhashed_function pulling out a hash corresponding to a common token. When they are different, the min has pulled out hashes corresponding to different tokens. From the same reasoning as the simple label encoding example above, one has\n",
    "\n",
    "$P[\\textrm{minhash}(t(d_x)) = \\textrm{minhash}(t(d_y))] = J_{sim}(d_x, d_y)$\n",
    "\n",
    "where (under a slight abuse of notation), the minhash function on a document is a random variable, and the same random hash is used on $d_x$ and $d_y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ce86a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empirical probability of minhashes being the same = 0.385\n"
     ]
    }
   ],
   "source": [
    "n_sample = 1000\n",
    "common_label_hits = 0\n",
    "for i_sample in range(n_sample):\n",
    "    # generating n_sample minhash integers per document, which can be thought of\n",
    "    # as generating a (minhash) vector of n_sample integers per document\n",
    "    minhashed_docs = get_minhashed_docs(tokenized_docs)\n",
    "    if minhashed_docs[0] == minhashed_docs[1]:\n",
    "        common_label_hits += 1\n",
    "print(f\"empirical probability of minhashes being the same = {common_label_hits/n_sample}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09e24ab",
   "metadata": {},
   "source": [
    "In the above, the minhashing operation has been performed over only 2 documents, but there is nothing to stop running the operation over $N$ documents to generate a minhash for each doc, and then check for equalities in the resulting minhashes.\n",
    "\n",
    "Each set of minhashes is a realisation of a random variable (a choice of randomly selected $a, b$ above). In this sense, the neighbour checking is statistical, and only an approximate nearest neighbour.\n",
    "\n",
    "The hashing process is also a lossy operation. Taking n_sample to be small yields a poor approximation to the Jaccard similarity. However, taking n_sample to be large means that a large amount of processing has to occur, yielding long minhash vectors, partially negating the benefit if using minhashing for computational and storage gain. The choice of minhash vector length (i.e. n_sample) really feeds into the LSH scheme, but generally does reflect 'how approximate' the approximate nearest neighbour approach will be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96aaf78b",
   "metadata": {},
   "source": [
    "## Locality-sensitive hashing\n",
    "\n",
    "In the above analysis, there is a function that takes tokenized documents to minhash vectors. These minhash vectors are statistical samples representing the Jaccard similarity between the referenced documents, and can be used to (statistically) estimate the Jaccard similarity.\n",
    "\n",
    "However, to declare if a pair of documents are neighbours, one must apply a threshold to the Jaccard similarity. This is accomplished using 'rows and bands' over the minhash vectors. The underlying idea is that requiring more of the minhashes (over the minhash vector) to be equal puts a stronger constraint on saying when two documents are neighbours, and therefore corresponds to a tighter/smaller threshold on the Jaccard distance.\n",
    "\n",
    "The LSH rows-and-bands approach is in fact a general approach for applying a threshold to a hashing scheme, and the same idea can be applied to minhash vectors, simhash vectors or Hamming-bit-wise vectors. The minhash vectors (of length n_samples) are split into 'b' bands, with each band containing 'r' rows (i.e. n_samples = $r \\times b$). Whether or not a pair of documents are considered as neighbours then comes down to whether or not there is a band where all minhashes match [[9]](http://csce.uark.edu/~lz006/course/2022fall/05-lsh.pdf) (in the digram below, each vertical slice corresponds to a minhash vector, and the horizontal axis scrolls over documents).\n",
    "\n",
    "![rows and bands pic](rows-and-bands.jpg)\n",
    "\n",
    "In the extreme cases, where there is only one band, $b=1$, this corresponds to the case where all minhashes across the minhash vector need to match for a pair of documents to be neighbours; this would correspond to a tight/small threshold on the Jaccard distance. On the other hand, when there is only one row per band, $b=$ n_sample, then only one minhash across a pair of minhash vectors needs to match; this would correspond to a loose/large threshold on the Jaccard distance. Remember that each row corresponds to a particular realisation of a minhash random variable (in the sense that they correspond to a particular value of 'a' and 'b' used in the hash), and so different bands have no shared context, i.e. the numbers in, say, band 3 have nothing to do with the numbers in band 2.\n",
    "\n",
    "The calculations needed can be captured by performing another hashing operation that takes the minhash vectors and converts them to LSH vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9dcbedc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minhashed docs: \n",
      "[921, 1044]\n",
      "[527, 2352]\n",
      "[951, 2976]\n",
      "[1009, 1009]\n",
      "[2833, 4665]\n",
      "[2443, 962]\n",
      "[1916, 417]\n",
      "[1344, 4563]\n",
      "[1598, 1667]\n",
      "[1641, 1641]\n",
      "[493, 2931]\n",
      "[2788, 4329]\n",
      "\n",
      "\n",
      "lsh-ed docs: \n",
      "[6036972117908916807, 2203983803339760343]\n",
      "[3506039047526209139, 6873368170506424222]\n",
      "[3033274914972165262, 4187685413567971262]\n"
     ]
    }
   ],
   "source": [
    "def get_lsh_docs_from_minhashed_docs(minhashed_docs, n_bands, n_rows):\n",
    "    assert len(minhashed_docs) == n_bands * n_rows\n",
    "    lsh_docs = []\n",
    "    for i_band in range(n_bands):\n",
    "        a, b = random.randint(1, 2 ** 31), random.randint(1, 2 ** 31)\n",
    "        buckets = 2 ** 63  # so hash will fit in an int64\n",
    "        these_docs = minhashed_docs[(i_band * n_rows):(i_band * n_rows) + n_rows]\n",
    "        this_band_lsh_docs = []\n",
    "        for i_doc in range(len(these_docs[0])):\n",
    "            # for this doc, make a single number, via string concatenation,\n",
    "            # out of all the minhashes in the band; this will concatenate n_rows integers\n",
    "            num_as_str = \"\"\n",
    "            for i_row in range(n_rows):\n",
    "                num_as_str += str(these_docs[i_row][i_doc])\n",
    "            # make a single hash out of the concatenated number, so that it fits into\n",
    "            # a reasonably sized number\n",
    "            this_band_lsh_docs.append((a * int(num_as_str) + b) % buckets)\n",
    "        lsh_docs.append(this_band_lsh_docs)\n",
    "    return lsh_docs\n",
    "            \n",
    "# 2 docs\n",
    "minhashed_docs = [get_minhashed_docs(tokenized_docs) for _ in range(12)]  # 12 minhashes per doc\n",
    "print(*([\"minhashed docs: \"] + minhashed_docs), sep=\"\\n\")\n",
    "lshed_docs = get_lsh_docs_from_minhashed_docs(minhashed_docs, 3, 4)  # 3 bands, 4 rows in each band\n",
    "print(*([\"\", \"\", \"lsh-ed docs: \"] + lshed_docs), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7895d493",
   "metadata": {},
   "source": [
    "The question of whether or not any documents are neighbours then comes down to looking at the LSH vectors, and seeing if, within band, there are any matching LSH numbers that match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dfae0ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_neighbours_of_lsh_doc(i_doc, lshed_docs):\n",
    "    neighbour_docs_indices = set()\n",
    "    for lsh_row in lshed_docs:\n",
    "        query_lsh = lsh_row[i_doc]\n",
    "        matching_docs = {x[0] for x in enumerate(lsh_row) if x[1] == query_lsh}\n",
    "        neighbour_docs_indices |= matching_docs\n",
    "    return neighbour_docs_indices\n",
    "\n",
    "get_neighbours_of_lsh_doc(0, lshed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0c306a",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] https://en.wikipedia.org/wiki/Metric_space\n",
    "\n",
    "[2] https://en.wikipedia.org/wiki/Locality-sensitive_hashing\n",
    "\n",
    "[3] https://en.wikipedia.org/wiki/MinHash\n",
    "\n",
    "[4] https://en.wikipedia.org/wiki/SimHash\n",
    "\n",
    "[5] https://www.youtube.com/watch?v=96WOGPUgMfw\n",
    "\n",
    "[6] https://www.youtube.com/watch?v=_1D35bN95Go\n",
    "\n",
    "[7] https://arxiv.org/abs/1809.04052 - Ryan Moulton, Yunjiang Jiang, \"Maximally Consistent Sampling and the Jaccard Index of Probability Distributions\"\n",
    "\n",
    "[8] http://infolab.stanford.edu/~ullman/mmds/book0n.pdf - Jure Leskovec, Anand Rajaraman, Jeffrey D. Ullman, \"Mining of massive datasets\"\n",
    "\n",
    "[9] http://csce.uark.edu/~lz006/course/2022fall/05-lsh.pdf - Jure Leskovec, \"Finding Similar Items &\n",
    "Locality Sensitive Hashing\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2029d84a",
   "metadata": {},
   "source": [
    "## Appendix - Jaccard distance triangle inequality\n",
    "\n",
    "Using\n",
    "\n",
    "$J_{dist}(A, B) = 1 - \\frac{|A \\cap B|}{|A \\cup B|} = \\frac{|A \\cup B| - |A \\cap B|}{|A \\cup B|}$\n",
    "\n",
    "many of the properties required for $J_{dist}$ are easy to see, e.g.\n",
    "- $J_{dist}(A, A) = 0$\n",
    "- $J_{dist}(A, B) > 0$ for $A\\neq B$\n",
    "- $J_{dist}(A, B) = J_{dist}(B, A)$\n",
    "\n",
    "However the triangle inequality is not quite so obvious. Using the statistical representation of the Jaccard distance as a minhash H, this becomes a little easier using properties of probabilities [[7]](https://arxiv.org/abs/1809.04052). First, note that\n",
    "\n",
    "$ J_{sim}(A, B) = P[H(A) = H(B)]$\n",
    "\n",
    "$ J_{dist}(A, B) = 1 - P[H(A) = H(B)] = P[H(A) \\neq H(B)]$\n",
    "\n",
    "Introducing a third set $C$, one has\n",
    "\n",
    "$ P[H(A) = H(B)] \\geq P[H(A) = H(B) = H(C)] = P[H(A) = H(C) \\cap H(C) = H(B)]$\n",
    "\n",
    "and therefore, by negating the above statement,\n",
    "\n",
    "$ P[H(A) \\neq H(B)] \\leq P[H(A) \\neq H(C) \\cup H(C) \\neq H(B)]$.\n",
    "\n",
    "Now, by $P[X\\cup Y] = P[X] + P[Y] - P[X\\cap Y]$, \n",
    "\n",
    "$P[H(A) \\neq H(C) \\cup H(C) \\neq H(B)] = P[H(A) \\neq H(C)] + P[H(C) \\neq H(B)] - P[H(A) \\neq H(C) \\cap H(C) \\neq H(B)]$\n",
    "\n",
    "and therefore \n",
    "\n",
    "$P[H(A) \\neq H(C) \\cup H(C) \\neq H(B)] \\leq P[H(A) \\neq H(C)] + P[H(C) \\neq H(B)] $\n",
    "\n",
    "Combining the two inequalities above, one finds that\n",
    "\n",
    "$ P[H(A) \\neq H(B)] \\leq P[H(A) \\neq H(C)] + P[H(C) \\neq H(B)]$\n",
    "\n",
    "which is the same as\n",
    "\n",
    "$ J_{dist}(A, B) \\leq J_{dist}(A, C) + J_{dist}(C, B) $\n",
    "\n",
    "hence expressing the triangle inequality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
